# -*- coding: utf-8 -*-
"""MLM Project DK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w7NYrK4ngY7hrfIdBAQcSOiJ_BzHtrbG
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
project_df = pd.read_csv('/content/gdrive/MyDrive/vehicle_price_prediction.csv')

"""Basic information regarding the dataset"""

project_df.info()

project_df

import numpy as np

project_df['id'] = np.arange(1, len(project_df) + 1)

project_df

null_counts = project_df.isnull().sum()
columns_with_nulls = null_counts[null_counts > 0]

if not columns_with_nulls.empty:
    print("Number of null records per column:")
    display(columns_with_nulls)
else:
    print("No columns with null values found.")

project_df.drop('accident_history', axis=1, inplace=True)

null_counts = project_df.isnull().sum()
columns_with_nulls = null_counts[null_counts > 0]

if not columns_with_nulls.empty:
    print("Number of null records per column:")
    display(columns_with_nulls)
else:
    print("No columns with null values found.")

rows_with_nulls = project_df[project_df.isnull().any(axis=1)]

if not rows_with_nulls.empty:
    num_null_columns_per_row = rows_with_nulls.isnull().sum(axis=1)
    print("Number of null columns per row for rows containing nulls:")
    display(num_null_columns_per_row)
    print(f"\nTotal number of rows with at least one null value: {len(rows_with_nulls)}")
else:
    print("No rows with any null values found in the DataFrame.")

project_df.info()

"""Divide the dataframe into categorical and non categorical subset for detailed statistics"""

categorical_cols = project_df.select_dtypes(include='object').columns.tolist()
# Add the 'id' column to the categorical columns list
categorical_cols.append('id')

non_categorical_cols = project_df.select_dtypes(exclude='object').columns.tolist()
# The 'id' column (int64) is already included in non_categorical_cols by default
# Ensure 'id' is present, although it should be due to select_dtypes(exclude='object')
if 'id' not in non_categorical_cols:
    non_categorical_cols.append('id')

categorical_df = project_df[categorical_cols]
non_categorical_df = project_df[non_categorical_cols]

categorical_df

non_categorical_df

display(non_categorical_df.describe())

print("Generating detailed statistics for categorical columns...")

# Exclude 'id' from categorical analysis as it's a unique identifier
categorical_cols_for_stats = [col for col in categorical_df.columns if col != 'id']

for col in categorical_cols_for_stats:
    print(f"\n--- Column: {col} ---")

    # Frequency of each category
    frequency = categorical_df[col].value_counts()
    print("Frequency of each category:")
    display(frequency)

    # Proportion of each category
    proportion = categorical_df[col].value_counts(normalize=True)
    print("\nProportion of each category:")
    display(proportion)

    # Mode (most frequent category)
    mode_val = categorical_df[col].mode().tolist()
    print(f"\nMode(s): {', '.join(map(str, mode_val))}")
    print("---------------------------------")

combined_df = pd.merge(categorical_df, non_categorical_df, on='id', how='inner')

combined_df

import plotly.express as px
import plotly.graph_objects as go

# Group by make and calculate average price
avg_price_by_make = combined_df.groupby('make')['price'].mean().reset_index().sort_values(by='price', ascending=False)

# Create a bar chart for average price by make
fig1 = px.bar(avg_price_by_make.head(15), x='make', y='price',
             title='Top 15 Car Makes by Average Price',
             labels={'make': 'Car Make', 'price': 'Average Price ($)'},
             hover_name='make',
             color='price',
             color_continuous_scale=px.colors.sequential.Viridis)

fig1.update_layout(xaxis_title='Car Make', yaxis_title='Average Price ($)', template='plotly_white')
fig1.show()

# Create a box plot to show price distribution across different body types
fig2 = px.box(combined_df, x='body_type', y='price',
             title='Price Distribution by Body Type',
             labels={'body_type': 'Body Type', 'price': 'Price ($)'},
             color='body_type',
             points='outliers' # Show outliers
            )

fig2.update_layout(xaxis_title='Body Type', yaxis_title='Price ($)', template='plotly_white')
fig2.show()

import plotly.express as px

# Scatter plot for Mileage vs. Price
fig_mileage_price = px.scatter(combined_df, x='mileage', y='price',
                               title='Mileage vs. Price',
                               labels={'mileage': 'Mileage', 'price': 'Price ($)'},
                               opacity=0.5,
                               hover_data=['make', 'model'])
fig_mileage_price.update_layout(xaxis_title='Mileage', yaxis_title='Price ($)', template='plotly_white')
fig_mileage_price.show()

# Scatter plot for Engine HP vs. Price
fig_enginehp_price = px.scatter(combined_df, x='engine_hp', y='price',
                                title='Engine Horsepower (HP) vs. Price',
                                labels={'engine_hp': 'Engine Horsepower', 'price': 'Price ($)'},
                                opacity=0.5,
                                hover_data=['make', 'model'])
fig_enginehp_price.update_layout(xaxis_title='Engine Horsepower', yaxis_title='Price ($)', template='plotly_white')
fig_enginehp_price.show()

# Box plot for Owner Count vs. Price
fig_owner_count_price = px.box(combined_df, x='owner_count', y='price',
                                title='Price Distribution by Owner Count',
                                labels={'owner_count': 'Number of Owners', 'price': 'Price ($)'},
                                color='owner_count',
                                points='outliers')
fig_owner_count_price.update_layout(xaxis_title='Number of Owners', yaxis_title='Price ($)', template='plotly_white')
fig_owner_count_price.show()

# Scatter plot for Vehicle Age vs. Price
fig_vehicleage_price = px.scatter(combined_df, x='vehicle_age', y='price',
                                  title='Vehicle Age vs. Price',
                                  labels={'vehicle_age': 'Vehicle Age (Years)', 'price': 'Price ($)'},
                                  opacity=0.5,
                                  hover_data=['make', 'model'])
fig_vehicleage_price.update_layout(xaxis_title='Vehicle Age', yaxis_title='Price ($)', template='plotly_white')
fig_vehicleage_price.show()

import plotly.express as px

# Scatter plot for Mileage per Year vs. Price
fig_mileage_per_year_price = px.scatter(combined_df, x='mileage_per_year', y='price',
                                        title='Mileage per Year vs. Price',
                                        labels={'mileage_per_year': 'Mileage per Year', 'price': 'Price ($)'},
                                        opacity=0.5,
                                        hover_data=['make', 'model'])
fig_mileage_per_year_price.update_layout(xaxis_title='Mileage per Year', yaxis_title='Price ($)', template='plotly_white')
fig_mileage_per_year_price.show()

X = combined_df.drop(['price', 'id'], axis=1)
y = combined_df['price']

categorical_features = X.select_dtypes(include=['object']).columns
numerical_features = X.select_dtypes(exclude=['object']).columns

print("Features (X) and Target (y) separated.")
print(f"Categorical features identified: {list(categorical_features)}")
print(f"Numerical features identified: {list(numerical_features)}")

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Apply Label Encoding to categorical features
le = LabelEncoder()
for col in categorical_features:
    X[col] = le.fit_transform(X[col])

print("Categorical features encoded using LabelEncoder.")

# Apply Min-Max Scaling to all features in X
# This includes the newly encoded categorical features and the original numerical features
scaler = MinMaxScaler()
X_processed = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)

print("All features scaled using MinMaxScaler.")
print("Processed features stored in X_processed.")

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

print("Data successfully split into training and testing sets.")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""**Reasoning**:
I need to sample the training data (`X_train` and `y_train`) to 10% of its current size to address memory issues, as specified in the subtask. I will use `train_test_split` with `train_size=0.1`.


"""

from sklearn.model_selection import train_test_split

# Sample 10% of the training data
X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)

print("Sampled training data shapes:")
print(f"X_train_sampled shape: {X_train_sampled.shape}")
print(f"y_train_sampled shape: {y_train_sampled.shape}")

from sklearn.ensemble import RandomForestRegressor

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
print("RandomForestRegressor instantiated.")

rf_regressor.fit(X_train_sampled, y_train_sampled)
print("RandomForestRegressor trained on sampled data.")

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

y_pred = rf_regressor.predict(X_test)

# Calculate evaluation metrics
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f"Model Performance on Original Test Set:")
print(f"R-squared (R2): {r2:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")

feature_importances = rf_regressor.feature_importances_
features = X_processed.columns

importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importances
})

importance_df = importance_df.sort_values(by='Importance', ascending=False)
print("Feature importances extracted and sorted.")
display(importance_df.head())

fig_importances = px.bar(importance_df.head(10), x='Feature', y='Importance',
                         title='Top 10 Feature Importances in Predicting Car Prices',
                         labels={'Feature': 'Feature', 'Importance': 'Importance Score'},
                         color='Importance',
                         color_continuous_scale=px.colors.sequential.Plasma)
fig_importances.update_layout(xaxis_title='Feature', yaxis_title='Importance Score', template='plotly_white')
fig_importances.show()

"""## Summary:

### Data Analysis Key Findings
*   **Data Split Discrepancy**: The initial data split resulted in a 60/40 ratio for training and testing (600,000 samples for training, 400,000 for testing), despite the intention to perform an 80/20 split.
*   **Sampled Training Data**: To manage computational resources, the `RandomForestRegressor` model was trained on a 10% sample of the available training data, which amounted to 60,000 samples.
*   **Model Performance**: When evaluated on the original, unsampled test set (400,000 samples), the `RandomForestRegressor` model achieved a strong predictive performance:
    *   R-squared (R2): 0.9684
    *   Mean Absolute Error (MAE): 1624.82
    *   Mean Squared Error (MSE): 5879239.50
*   **Top Feature Importances**: The analysis of feature importances revealed that `engine_hp`, `year`, `vehicle_age`, `mileage`, and `make` were the most significant factors in predicting car prices.

### Insights or Next Steps
*   The high R-squared value of 0.9684, even with a sampled training dataset, suggests that the `RandomForestRegressor` is highly effective at explaining the variance in car prices. This indicates that the selected features are strong predictors.
*   Given the successful performance on sampled data, a next step could involve exploring strategies to train the model on a larger portion or the entirety of the training data (if computational resources allow) to potentially further enhance accuracy and robustness, or to perform hyperparameter tuning to optimize the model's performance on the existing sampled data.

### Building the Linear Regression Model

Let's implement a `LinearRegression` model. Linear Regression is a fundamental statistical model that models the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). It assumes a linear relationship between the input features and the target variable.
"""

from sklearn.model_selection import train_test_split

# Sample 10% of the training data
X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)

print("Sampled training data shapes:")
print(f"X_train_sampled shape: {X_train_sampled.shape}")
print(f"y_train_sampled shape: {y_train_sampled.shape}")

from sklearn.linear_model import LinearRegression

# Initialize the Linear Regression model
linear_regressor = LinearRegression()
print("LinearRegression instantiated.")

# Train the model on the sampled training data
linear_regressor.fit(X_train_sampled, y_train_sampled)
print("LinearRegression trained on sampled data.")

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Make predictions on the test set
y_pred_lr = linear_regressor.predict(X_test)

# Calculate evaluation metrics
r2_lr = r2_score(y_test, y_pred_lr)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
mse_lr = mean_squared_error(y_test, y_pred_lr)

print(f"Linear Regression Model Performance on Original Test Set:")
print(f"R-squared (R2): {r2_lr:.4f}")
print(f"Mean Absolute Error (MAE): {mae_lr:.2f}")
print(f"Mean Squared Error (MSE): {mse_lr:.2f}")



"""### Applying Logistic Regression (with price binarization)

As previously discussed, Logistic Regression is designed for classification tasks. To apply it to our `price` variable, which is continuous, we first need to transform `price` into a categorical target. I'll create three price categories: 'Low', 'Medium', and 'High' based on quantiles. This demonstrates how Logistic Regression can be used in a scenario derived from the continuous target.
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Create a categorical target from the continuous 'price' for Logistic Regression
# Using quantiles to define 'Low', 'Medium', 'High' price categories
price_bins = pd.qcut(y, q=3, labels=['Low', 'Medium', 'High'])

# Split the binned target variable into training and testing sets
# Ensure the split aligns with X_train_sampled and X_test
# We'll use the same indices as y_train_sampled and y_test to ensure data consistency

# For training, we need to get the corresponding binned prices for the sampled y_train
y_train_binned = pd.qcut(y_train, q=3, labels=['Low', 'Medium', 'High'])
y_train_binned_sampled = y_train_binned.loc[y_train_sampled.index]

# For testing, we use the binned prices corresponding to the original y_test
y_test_binned = pd.qcut(y_test, q=3, labels=['Low', 'Medium', 'High'])

print("Categorical price target created.")
print(f"Distribution of binned prices (training sampled):\n{y_train_binned_sampled.value_counts(normalize=True)}")
print(f"Distribution of binned prices (test):\n{y_test_binned.value_counts(normalize=True)}")

# Initialize and train the Logistic Regression model
# Set max_iter for convergence, and solver to 'liblinear' for smaller datasets or 'lbfgs' for larger ones.
# 'saga' also works well with large datasets and handles multinomial loss.
logistic_regressor = LogisticRegression(max_iter=1000, random_state=42, solver='saga', n_jobs=-1)
print("LogisticRegression instantiated.")

logistic_regressor.fit(X_train_sampled, y_train_binned_sampled)
print("LogisticRegression trained on sampled data.")

# Make predictions on the test set with the binned target
y_pred_lr_binned = logistic_regressor.predict(X_test)

# Evaluate the model
accuracy_lr = accuracy_score(y_test_binned, y_pred_lr_binned)
report_lr = classification_report(y_test_binned, y_pred_lr_binned)

print(f"Logistic Regression Model Performance on Original Test Set (Categorical Price):")
print(f"Accuracy: {accuracy_lr:.4f}")
print("\nClassification Report:\n")
print(report_lr)